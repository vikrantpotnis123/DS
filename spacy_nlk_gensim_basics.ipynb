{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_nlk_gensim_basics.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgeXsDGxazzZ4oxQrLSs/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikrantpotnis123/DS/blob/master/spacy_nlk_gensim_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ItpZUL6ggQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBZL_m30-wv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5dYfetQivgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNmEMBoWMV38",
        "colab_type": "text"
      },
      "source": [
        "### **Word Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9urkhqeF2ryx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word tokenization\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "pp.pprint(token_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1niPGXC6L3JL",
        "colab_type": "text"
      },
      "source": [
        "### **Pipeline**\n",
        "Print token (text), label, lemma, pos, dep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWXqOuqYAb0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "    \"Apple is looking at buying U.K. startup for $1 billion\",\n",
        "    \"Manchester United isn't looking to sign any forward.\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    # Do something with the doc here\n",
        "    pp.pprint(doc)\n",
        "    pp.pprint([(ent.text, ent.label_, ent.lemma_) for ent in doc.ents])\n",
        "    pp.pprint([(tok.text, tok.pos_, tok.dep_) for tok in doc]) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrIHlrYfMCYa",
        "colab_type": "text"
      },
      "source": [
        "### **Sentencizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUfNhpyVLzDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence tokenization\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "# Create the pipeline 'sentencizer' component\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "doc = nlp(text)\n",
        "\n",
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "pp.pprint(sents_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4WeJ37qPN7x",
        "colab_type": "text"
      },
      "source": [
        "###  **Stop words**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFu_yiDrPUST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "pp.pprint(sp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA57OQjFblxr",
        "colab_type": "text"
      },
      "source": [
        "### **NLTK stemmer**\n",
        "\n",
        "Stemming refers to reducing a word to its root form.\n",
        "\n",
        "spaCy doesn't contain any function for stemming as it relies on **lemmatization** only. \n",
        "\n",
        "There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpPUroBKbrRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "for token in ['compute', 'computer', 'computed', 'computing', 'victor', 'victory', 'victim', 'victorious', 'victimized' , 'victimize']:\n",
        "    pp.pprint(token + ' --> ' + stemmer.stem(token))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ajOZ4LEmY2z",
        "colab_type": "text"
      },
      "source": [
        "### **GenSim**\n",
        "\n",
        "1.   Document: some text.\n",
        "2.   Corpus: a collection of documents.\n",
        "3.   Vector: a mathematically convenient representation of a document.\n",
        "4.   Model: an algorithm for transforming vectors from one representation to another\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH7MAQVlmdoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(sp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otsiw1AjAdqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "def download_wiki(url, debug = False):\n",
        "  wiki_data =  urllib.request.urlopen(url)\n",
        "  wiki_page = wiki_data.read()\n",
        "  parsed_wiki = bs.BeautifulSoup(wiki_page, 'lxml')\n",
        "  paras = parsed_wiki.find_all('p')\n",
        "  wiki_text = \"\"\n",
        "  for p in paras:\n",
        "    wiki_text +=  p.text\n",
        "    \n",
        "  # Remove square brackets, extra spaces\n",
        "  wiki_text = re.sub(r'\\[[0-9]*\\]', ' ', wiki_text)\n",
        "  wiki_text = re.sub(r'\\s+', ' ', wiki_text)\n",
        "\n",
        "  # Removing special characters and digits\n",
        "  if debug :\n",
        "    pp.pprint(wiki_text[0:100])\n",
        "  formatted_wiki_text = re.sub('[^a-zA-Z]', ' ', wiki_text)\n",
        "  \n",
        "  if debug:\n",
        "    pp.pprint(formatted_wiki_text[0:100])\n",
        "  formatted_wiki_text = re.sub(r'\\s+', ' ', formatted_wiki_text)\n",
        "  return formatted_wiki_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qRS9AhRniJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_corpus = download_wiki(\"https://en.wikipedia.org/wiki/Abraham_Lincoln\")\n",
        "# pp.pprint(text_corpus)\n",
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n",
        "# remove common words and tokenize\n",
        "stoplist = sp.Defaults.stop_words\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "pprint.pprint(corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BSCLHGQBNdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a transofrm\n",
        "from gensim import models\n",
        "\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "for doc in corpus_tfidf:\n",
        "    print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qOPG71hEZ7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}