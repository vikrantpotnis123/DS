{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_cnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1tHIlIS5aV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this tutorial demoes a simple CNN\n",
        "# https://algorithmia.com/blog/convolutional-neural-nets-in-pytorch\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    #Our batch shape for input x is (3, 32, 32)\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        #Input channels = 3, output channels = 18\n",
        "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        #4608 input features, 64 output features (see sizing flow below)\n",
        "        self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)\n",
        "        \n",
        "        #64 input features, 10 output features for our 10 defined classes\n",
        "        self.fc2 = torch.nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #Computes the activation of the first convolution\n",
        "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        \n",
        "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        #Reshape data to input to the input layer of the neural net\n",
        "        #Size changes from (18, 16, 16) to (1, 4608)\n",
        "        #Recall that the -1 infers this dimension from the other given dimension\n",
        "        x = x.view(-1, 18 * 16 *16)\n",
        "        \n",
        "        #Computes the activation of the first fully connected layer\n",
        "        #Size changes from (1, 4608) to (1, 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        #Computes the second fully connected layer (activation applied later)\n",
        "        #Size changes from (1, 64) to (1, 10)\n",
        "        x = self.fc2(x)\n",
        "        return(x)\n",
        "\n",
        "def outputSize(in_size, kernel_size, stride, padding):\n",
        "  output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
        "  return(output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}